"""
Command-line interface for the info aggregation module.

Usage:
    python -m src.aggregator.cli --keywords "æ¸¸æˆå¼€å‘" "ç‹¬ç«‹æ¸¸æˆ" --providers bocha tavily
"""

import argparse
import asyncio
import logging
import sys
from pathlib import Path
from typing import List

from src.aggregator.schemas import QueryRequest
from src.aggregator.engine import AggregationEngine
from src.aggregator.io import CSVWriter

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)


def parse_args() -> argparse.Namespace:
    """Parse command-line arguments."""
    parser = argparse.ArgumentParser(
        description='ä¿¡æ¯æ•´ç†æ¨¡å— - èšåˆæœç´¢å¤šä¸ªå…³é”®è¯è·¨å¤šä¸ªæä¾›å•†',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
Examples:
  # ä½¿ç”¨ä¸¤ä¸ªæä¾›å•†æŸ¥è¯¢å•ä¸ªå…³é”®è¯
  python -m src.aggregator.cli --keywords "æ·±åœ³ç‹¬ç«‹æ¸¸æˆ" --providers bocha tavily
  
  # æŸ¥è¯¢å¤šä¸ªå…³é”®è¯å¹¶ä¿å­˜åˆ°æŒ‡å®šæ–‡ä»¶
  python -m src.aggregator.cli --keywords "æ¸¸æˆå¼€å‘" "ç‹¬ç«‹æ¸¸æˆ" "Godotå¼•æ“" \\
      --providers bocha tavily --filename game_dev_results.csv
  
  # ä»…ä½¿ç”¨ Bocha æä¾›å•†
  python -m src.aggregator.cli --keywords "AI Agent" --providers bocha
  
  # å¯ç”¨ LLM å…³é”®è¯å˜ä½“ç”Ÿæˆï¼ˆæé«˜å¬å›ç‡ï¼‰
  python -m src.aggregator.cli --keywords "æ·±åœ³ç‹¬ç«‹æ¸¸æˆ" --generate-variants
  
  # æŒ‡å®šæ¯ä¸ªæä¾›å•†è¿”å›æœ€å¤š 20 æ¡ç»“æœ
  python -m src.aggregator.cli --keywords "æ¸¸æˆå¼€å‘" --max-results-per-provider 20
  
  # è·å–å°½å¯èƒ½å¤šçš„ç»“æœï¼ˆBochaæœ€å¤š50æ¡ï¼ŒTavilyæœ€å¤š20æ¡ï¼‰
  python -m src.aggregator.cli --keywords "AI Agent" --max-results-per-provider 100
        '''
    )
    
    parser.add_argument(
        '--keywords',
        nargs='+',
        required=True,
        help='è¦æœç´¢çš„å…³é”®è¯åˆ—è¡¨'
    )
    
    parser.add_argument(
        '--providers',
        nargs='+',
        choices=['bocha', 'tavily'],
        default=['bocha', 'tavily'],
        help='è¦ä½¿ç”¨çš„æä¾›å•† (é»˜è®¤: ä¸¤è€…éƒ½ç”¨)'
    )
    
    parser.add_argument(
        '--out',
        type=str,
        default='data/results',
        help='CSV è¾“å‡ºç›®å½• (é»˜è®¤: data/results)'
    )
    
    parser.add_argument(
        '--filename',
        type=str,
        default='results.csv',
        help='CSV æ–‡ä»¶å (é»˜è®¤: results.csv)'
    )
    
    parser.add_argument(
        '--no-cache',
        action='store_true',
        help='ç¦ç”¨ç¼“å­˜'
    )
    
    parser.add_argument(
        '--generate-variants',
        action='store_true',
        help='ä½¿ç”¨ LLM ç”Ÿæˆå…³é”®è¯å˜ä½“ä»¥æé«˜å¬å›ç‡ (éœ€è¦é…ç½® SMALL_LLM_URL)'
    )
    
    parser.add_argument(
        '--max-results-per-provider',
        type=int,
        default=10,
        choices=range(1, 101),
        metavar='[1-100]',
        help='æ¯ä¸ªæä¾›å•†çš„æœ€å¤§è¿”å›æ¡æ•° (é»˜è®¤: 10, èŒƒå›´: 1-100, ä¼šæ ¹æ®APIé™åˆ¶è‡ªåŠ¨è°ƒæ•´)'
    )
    
    parser.add_argument(
        '--max-concurrent',
        type=int,
        default=5,
        help='æœ€å¤§å¹¶å‘å…³é”®è¯æ•° (é»˜è®¤: 5)'
    )
    
    parser.add_argument(
        '--cache-ttl',
        type=int,
        default=86400,
        help='ç¼“å­˜ TTL ç§’æ•° (é»˜è®¤: 86400 = 24å°æ—¶)'
    )
    
    parser.add_argument(
        '--verbose',
        '-v',
        action='store_true',
        help='è¯¦ç»†è¾“å‡º (DEBUG æ—¥å¿—çº§åˆ«)'
    )
    
    return parser.parse_args()


async def main():
    """Main CLI entry point."""
    args = parse_args()
    
    # Configure logging level
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    logger.info(f"ä¿¡æ¯æ•´ç†æ¨¡å— CLI å¯åŠ¨")
    logger.info(f"å…³é”®è¯: {args.keywords}")
    logger.info(f"æä¾›å•†: {args.providers}")
    
    # Handle keyword variant generation if requested
    keywords_to_search = args.keywords
    if args.generate_variants:
        logger.info("å¯ç”¨å…³é”®è¯å˜ä½“ç”Ÿæˆ...")
        try:
            from src.aggregator.keywords import generate_variants_for_keywords
            
            print("\nğŸ”„ æ­£åœ¨ç”Ÿæˆå…³é”®è¯å˜ä½“...")
            keywords_to_search = await generate_variants_for_keywords(args.keywords)
            
            print(f"âœ“ å·²ç”Ÿæˆ {len(keywords_to_search)} ä¸ªå…³é”®è¯ï¼ˆåŒ…æ‹¬åŸå§‹å…³é”®è¯å’Œå˜ä½“ï¼‰")
            if args.verbose:
                print(f"  å…³é”®è¯åˆ—è¡¨: {', '.join(keywords_to_search[:10])}" + 
                      (f" ... (+{len(keywords_to_search)-10} more)" if len(keywords_to_search) > 10 else ""))
            print()
        except Exception as e:
            logger.error(f"å…³é”®è¯å˜ä½“ç”Ÿæˆå¤±è´¥: {e}")
            print(f"âš  å…³é”®è¯å˜ä½“ç”Ÿæˆå¤±è´¥ï¼Œå°†ä½¿ç”¨åŸå§‹å…³é”®è¯: {e}\n")
            keywords_to_search = args.keywords
    
    # Create request
    request = QueryRequest(
        keywords=keywords_to_search,
        providers=args.providers,
        max_results_per_provider=args.max_results_per_provider
    )
    
    # Initialize engine
    engine = AggregationEngine(
        max_concurrent_keywords=args.max_concurrent,
        cache_ttl=args.cache_ttl,
        use_cache=not args.no_cache
    )
    
    try:
        # Execute aggregation
        logger.info("å¼€å§‹èšåˆæŸ¥è¯¢...")
        print(f"ğŸ” æ­£åœ¨æŸ¥è¯¢ {len(keywords_to_search)} ä¸ªå…³é”®è¯...")
        response = await engine.aggregate(request)
        
        # Display summary
        print("\n" + "="*60)
        print(f"âœ“ æŸ¥è¯¢å®Œæˆï¼å…±æ‰¾åˆ° {response.total_count} æ¡ç»“æœ")
        print("="*60)
        
        for provider, results in response.by_provider.items():
            print(f"  {provider}: {len(results)} æ¡ç»“æœ")
        
        # Save to CSV
        if response.results:
            csv_writer = CSVWriter(output_dir=args.out)
            output_path = csv_writer.write_results(response.results, args.filename)
            
            print(f"\nğŸ“ ç»“æœå·²ä¿å­˜åˆ°:")
            print(f"   {output_path.absolute()}")
            print(f"   æ€»è¡Œæ•°: {len(response.results)}")
        else:
            logger.warning("æœªæ‰¾åˆ°ä»»ä½•ç»“æœï¼Œè·³è¿‡ CSV å†™å…¥")
            sys.exit(1)
        
        print("="*60 + "\n")
        
    except KeyboardInterrupt:
        logger.warning("ç”¨æˆ·ä¸­æ–­")
        sys.exit(130)
    except Exception as e:
        logger.error(f"èšåˆå¤±è´¥: {e}", exc_info=True)
        sys.exit(1)
    finally:
        await engine.close()


if __name__ == '__main__':
    asyncio.run(main())
